{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "75VOiRumbtCW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Text Generation with RNNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q-eZcyy_btCX",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kwc0o4bGbtCa"
   },
   "source": [
    "### 1 Dataset\n",
    "Define the path of the file, you want to read and train the model on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_-pWsSdWbtCb",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import urllib  # the lib that handles the url stuff\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for line in urllib.request.urlopen(\"https://raw.githubusercontent.com/GITenberg/Moby-Dick--Or-The-Whale_2701/master/2701.txt\"):\n",
    "    text += line.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ku1gyuOibtCd"
   },
   "source": [
    "\n",
    "#### Inspect the dataset\n",
    "Take a look at the first 250 characters in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hOoE3ZLwbtCe",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Pro\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ixxZ4yBqbtCh",
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luvcRu86btCj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2 Process the dataset for the learning task\n",
    "The task that we want our model to achieve is: given a character, or a sequence of characters, what is the most probable next character?\n",
    "\n",
    "To achieve this, we will input a sequence of characters to the model, and train the model to predict the output, that is, the following character at each time step. RNNs maintain an internal state that depends on previously seen elements, so information about all characters seen up until a given moment will be taken into account in generating the prediction.\n",
    "\n",
    "#### Vectorize the text\n",
    "Before we begin training our RNN model, we'll need to create a numerical representation of our text-based dataset. To do this, we'll generate two lookup tables: one that maps characters to numbers, and a second that maps numbers back to characters. Recall that we just identified the unique characters present in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7koc3Q2dbtCk",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# Create a mapping from indices to characters\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s04cWuBCbtCm"
   },
   "source": [
    "\n",
    "This gives us an integer representation for each character. Observe that the unique characters (i.e., our vocabulary) in the text are mapped as indices from 0 to len(unique). Let's take a peek at this numerical representation of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7Rbt2hfpbtCn",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  '\"' :   4,\n",
      "  '#' :   5,\n",
      "  '$' :   6,\n",
      "  '%' :   7,\n",
      "  '&' :   8,\n",
      "  \"'\" :   9,\n",
      "  '(' :  10,\n",
      "  ')' :  11,\n",
      "  '*' :  12,\n",
      "  ',' :  13,\n",
      "  '-' :  14,\n",
      "  '.' :  15,\n",
      "  '/' :  16,\n",
      "  '0' :  17,\n",
      "  '1' :  18,\n",
      "  '2' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dsh270g9btCp"
   },
   "source": [
    "\n",
    "\n",
    "We can also look at how the first part of the text is mapped to an integer representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "egRTUKhlbtCq",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The Project G' ---- characters mapped to int ---- > [50 67 64  2 46 77 74 69 64 62 79  2 37]\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fti3o5GHbtCs"
   },
   "source": [
    "#### Defining a method to encode one hot labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oef5BSBpbtCt",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((len(arr), n_labels))\n",
    "    for i, el in enumerate(arr):\n",
    "        one_hot[i][el] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxoJlIhmbtCw"
   },
   "source": [
    "\n",
    "#### Defining a method to make mini-batches for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1JNdq5YgbtCw",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length, vocab_len):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "\n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    start_idxs = list(range(len(arr) - 10 * seq_length))\n",
    "    random.shuffle(start_idxs)\n",
    "    for start_idx in start_idxs:\n",
    "        chunk = arr[start_idx:start_idx + seq_length]\n",
    "        x = one_hot_encode(chunk[:-1], vocab_len)\n",
    "        y = chunk[-1]\n",
    "        x_batch.append(x)\n",
    "        y_batch.append(y)\n",
    "        if len(x_batch) == batch_size:\n",
    "            yield torch.FloatTensor(x_batch), torch.LongTensor(y_batch)\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHoy0u0abtCz"
   },
   "source": [
    "\n",
    "## 3 The Recurrent Neural Network (RNN) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tkdc8Z6VbtC0"
   },
   "source": [
    "\n",
    "###### Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5wRmMM9kbtC0",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4b7JO6FbtC3"
   },
   "source": [
    "\n",
    "### Declaring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "k72_pm6obtC3",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, x_size, a_size, y_size):\n",
    "        super().__init__()\n",
    "        # Either w_ax or w_aa doesn't need a bias\n",
    "        self.w_ax = nn.Linear(x_size, a_size)\n",
    "        self.w_aa = nn.Linear(a_size, a_size, bias=False)\n",
    "        self.w_ya = nn.Linear(a_size, y_size)\n",
    "        self.a_size = a_size\n",
    "        \n",
    "    def forward(self, x, last_a):\n",
    "        '''Forward pass through the network\n",
    "        x is the input and `hidden` is the hidden/cell state .'''\n",
    "        a = nn.functional.relu(self.w_aa(last_a) + self.w_ax(x))\n",
    "        out = nn.functional.softmax(self.w_ya(a), dim=1)\n",
    "        return out, a\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        a = torch.zeros((batch_size, self.a_size))\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eKFZqqjw013I"
   },
   "outputs": [],
   "source": [
    "class LSTMCharRNN(nn.Module):\n",
    "    def __init__(self, y_size, a_size, x_size, n_layers=2,\n",
    "                 drop_prob = 0.5):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        '''TODO: define the layers you need for the model'''\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        '''TODO: Forward pass through the network\n",
    "        x is the input and `hidden` is the hidden/cell state .'''\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden_t\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        \n",
    "        #hidden = # TODO\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD6AFo9zbtC8"
   },
   "source": [
    "\n",
    "#### Declaring the train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "S2HKSmmAbtC9",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, data, vocab_len, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "\n",
    "        model: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    '''\n",
    "    model.train()\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data) * (1 - val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for e in range(epochs):\n",
    "        running_train_losses = []\n",
    "        running_val_losses = []\n",
    "        h = model.init_hidden(batch_size)\n",
    "\n",
    "        for x, y in tqdm(get_batches(data, batch_size, seq_length, vocab_len)):\n",
    "            # Only process on full batches\n",
    "            if x.shape[0] != batch_size:\n",
    "                continue\n",
    "            opt.zero_grad()\n",
    "            a = model.init_hidden(batch_size).to(device)\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            for char_pos in range(x.shape[1]):\n",
    "                inp = x[:, char_pos, :]\n",
    "                out, a = model(inp, a)\n",
    "            \n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running_train_losses.append(loss.item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(get_batches(val_data, batch_size, seq_length, vocab_len)):\n",
    "                if x.shape[0] != batch_size:\n",
    "                    continue\n",
    "                a = model.init_hidden(batch_size).to(device)\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                for char_pos in range(x.shape[1]):\n",
    "                    inp = x[:, char_pos, :]\n",
    "                    out, a = model(inp, a)\n",
    "\n",
    "                loss = criterion(out, y)\n",
    "                running_val_losses.append(loss.item())\n",
    "                \n",
    "        train_loss = np.mean(np.array(running_train_losses))\n",
    "        val_loss = np.mean(np.array(running_val_losses))\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {e + 1}/{epochs} => train_loss: {train_loss} val_loss: {val_loss}\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtlmj6sAbtC_"
   },
   "source": [
    "\n",
    "##### Defining a method to generate the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eTDAUosNbtC_",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, char, vocab_len, h=None):\n",
    "    ''' Given a character, predict the next character.\n",
    "        Returns the predicted character and the hidden state.\n",
    "    '''\n",
    "\n",
    "    # tensor inputs\n",
    "    x = np.array([[char2idx[char]]])\n",
    "    x = one_hot_encode(x, len(model.vocab))\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    if (train_on_gpu):\n",
    "        inputs = inputs.cuda()\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "    '''TODO: feed the current input into the model and generate output'''\n",
    "    output, h = model(inputs, h) # TODO\n",
    "\n",
    "    # get the character probabilities\n",
    "    p = F.softmax(out, dim=1).data.to(device)\n",
    "\n",
    "    # get top characters\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(model.vocab))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "    # select the likely next character with some element of randomness\n",
    "    p = p.numpy().squeeze()\n",
    "    char = np.random.choice(top_ch, p=p / p.sum())\n",
    "\n",
    "    # return the encoded value of the predicted char and the hidden state\n",
    "    return idx2char[char], h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a_I3GHqbtDB"
   },
   "source": [
    "\n",
    "#### Declaring a method to generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mS9UcwBXbtDC",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample(model, size, prime='The', top_k=None):\n",
    "    if (train_on_gpu):\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "\n",
    "    model.eval()  # eval mode\n",
    "\n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = model.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(model, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    for ii in range(size):\n",
    "        '''TODO: pass in the previous character and get a new one'''\n",
    "        # char, h = # TODO\n",
    "        chars.append(char)\n",
    "\n",
    "    model.train()\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viwhJM9RbtDE"
   },
   "source": [
    "\n",
    "#### Generate new Text using the RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7bc2y3DbtDE"
   },
   "source": [
    "###### Define and print the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mHbAsHB2btDF",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaRNN(\n",
      "  (w_ax): Linear(in_features=86, out_features=128, bias=True)\n",
      "  (w_aa): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (w_ya): Linear(in_features=128, out_features=86, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lstm_model = LSTMCharRNN(vocab, n_hidden, n_layers)\\nprint(lstm_model)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_model = VanillaRNN(x_size=len(vocab), a_size=128, y_size=len(vocab)).to(device)\n",
    "print(vanilla_model)\n",
    "\"\"\"lstm_model = LSTMCharRNN(vocab, n_hidden, n_layers)\n",
    "print(lstm_model)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-sU3cQIbtDI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "###### Declaring the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZJnyYkQlbtDJ",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "''''TODO: Try changing the hyperparameters in the network to see how it affects performance'''\n",
    "batch_size = 128\n",
    "seq_length = 20\n",
    "n_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46LJ32XHbtDM"
   },
   "source": [
    "\n",
    "##### Train the model and have fun with the generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bzc7Dz65FOPs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8840it [00:51, 173.04it/s]\n",
      "982it [00:03, 291.14it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 => train_loss: 4.168095596775211 val_loss: 4.148660471871525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8840it [00:50, 176.17it/s]\n",
      "982it [00:03, 300.54it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 => train_loss: 4.140311367074828 val_loss: 4.145475460895453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1515it [00:09, 175.18it/s]"
     ]
    }
   ],
   "source": [
    "van_train_losses, van_val_losses = train(vanilla_model, text_as_int, vocab_len=len(vocab), epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdBcUbv6FObG"
   },
   "outputs": [],
   "source": [
    "train(lstm_model, text_as_int, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNN_Assignment_13.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
