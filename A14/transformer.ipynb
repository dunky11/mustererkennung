{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Setting some hyperparameters\n",
    "\"\"\"\n",
    "# size of the query, key, value and z vectors \n",
    "# in the attention layer (64 was used in paper)\n",
    "ATTENTION_OUTPUT_DIM = 64\n",
    "WORD_EMBEDDING_DIM = 512\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def positional_encoding(seq_len, input_dim, device):\n",
    "    \"\"\" FROM https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51\n",
    "    TODO understand this.\n",
    "    \"\"\"\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(input_dim, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / 10000 ** (dim // input_dim)\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))\n",
    "\n",
    "class TwoLayerNN(nn.Module):\n",
    "    \"\"\" Two layer network, without activation function for the last layer like in the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=2048):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dense_2 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "        \n",
    "def scaled_dot_product_attention(queries, keys, values, mask=None):\n",
    "    scores = queries @ torch.transpose(keys, 1, 2)\n",
    "    if mask != None:\n",
    "        scores.masked_fill(mask == 0, -1e9)\n",
    "    scaled_scores = scores / np.sqrt(keys.shape[1])\n",
    "    softmax_scores = F.softmax(scaled_scores, dim=1)\n",
    "    z = softmax_scores @ values\n",
    "    return z\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.W_k = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.W_v = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        \n",
    "    def forward(self, queries_based, keys_based, values_based, mask=None):\n",
    "        queries = self.W_q(values_based)\n",
    "        keys = self.W_k(keys_based)\n",
    "        values = self.W_v(values_based)\n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        return attention\n",
    "    \n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads):\n",
    "        \"\"\"\n",
    "        input_dim : dim of the input, output will have same dimension\n",
    "        num_heads : number of self attention layers to concatenate, each will have a \n",
    "        query, key and value dimension of num_heads // input_dim\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.head_dim = input_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_layers = nn.ModuleList(\n",
    "            [AttentionLayer(input_dim, self.head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.W_o = nn.Linear(self.head_dim * num_heads, input_dim, bias=False)\n",
    "        \n",
    "    def forward(self, queries_based, keys_based, values_based, mask=None):\n",
    "        attentions = []\n",
    "        for attention_layer in self.attention_layers:\n",
    "            attention = attention_layer(queries_based, keys_based, values_based, mask)\n",
    "            attentions.append(attention)\n",
    "        z = torch.cat(attentions, dim=2)\n",
    "        output = self.W_o(z)\n",
    "        return z\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=8, dropout_p=0.1, ff_hidden_dim=2048):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttentionLayer(input_dim, num_heads)\n",
    "        self.feed_forward = TwoLayerNN(input_dim, ff_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.norm_1 = nn.LayerNorm(input_dim)\n",
    "        self.norm_2 = nn.LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, queries_based, keys_based, values_based):\n",
    "        attention = self.attention(queries_based, keys_based, values_based)\n",
    "        attention = self.dropout(attention)\n",
    "        z1 = self.norm_1(attention + values_based)\n",
    "        z2 = self.feed_forward(z1)\n",
    "        z2 = self.dropout(z2)\n",
    "        z2 = self.norm_2(z1 + z2)\n",
    "        return z2\n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=8, dropout_p=0.1, ff_hidden_dim=2048):\n",
    "        super().__init__()\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            input_dim, num_heads, dropout_p, ff_hidden_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.transformer_block(x, x, x)\n",
    "        return z\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dropout_p=0.1, ff_hidden_dim=2048):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.W_k = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.W_v = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.attention = MultiHeadAttentionLayer(input_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            input_dim, num_heads, dropout_p, ff_hidden_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_encoder, x_decoder, mask):\n",
    "        attention = self.attention(x_decoder, x_encoder, x_encoder, mask)\n",
    "        attention = self.dropout(attention)\n",
    "        z1 = self.norm(attention + x_decoder)\n",
    "        z2 = self.transformer_block(x_encoder, x_encoder, z1)\n",
    "        return z2\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_len, \n",
    "                 device, \n",
    "                 num_heads=8, \n",
    "                 embedding_dim=512, \n",
    "                 n_layers=6, \n",
    "                 dropout_p=0.1, \n",
    "                 ff_hidden_dim=2048):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_len, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [EncoderBlock(embedding_dim, num_heads, dropout_p, ff_hidden_dim) for i in range(n_layers)]\n",
    "        )\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.word_embedding(x)\n",
    "        pos_enc = positional_encoding(x.shape[1], self.embedding_dim, self.device)\n",
    "        x = x + pos_enc\n",
    "        x = self.dropout(x)\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_len, \n",
    "                 device, \n",
    "                 num_heads=8, \n",
    "                 embedding_dim=512, \n",
    "                 n_layers=6, \n",
    "                 dropout_p=0.1, \n",
    "                 ff_hidden_dim=2048):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_len, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [DecoderBlock(embedding_dim, num_heads, dropout_p, ff_hidden_dim) for i in range(n_layers)]\n",
    "        )\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_len)\n",
    "        \n",
    "        \n",
    "    def forward(self, x_encoder, x_decoder, mask):\n",
    "        x = self.word_embedding(x_decoder)\n",
    "        pos_enc = positional_encoding(x.shape[1], self.embedding_dim, self.device)\n",
    "        x = x + pos_enc\n",
    "        x = self.dropout(x)\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x_encoder, x, mask)\n",
    "        x = self.fc_out(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_len_src,\n",
    "                 vocab_len_trg,\n",
    "                 max_len,\n",
    "                 device, \n",
    "                 num_heads=8, \n",
    "                 embedding_dim=512, \n",
    "                 n_layers_encoder=6,\n",
    "                 n_layers_decoder=6,\n",
    "                 dropout_p_encoder=0.1,\n",
    "                 dropout_p_decoder=0.1, \n",
    "                 ff_hidden_dim_encoder=2048,\n",
    "                 ff_hidden_dim_decoder=2048):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_len_src, \n",
    "                               device, \n",
    "                               num_heads, \n",
    "                               embedding_dim, \n",
    "                               n_layers_encoder, \n",
    "                               dropout_p_encoder, \n",
    "                               ff_hidden_dim_encoder)\n",
    "        self.decoder = Decoder(vocab_len_trg, \n",
    "                               device, \n",
    "                               num_heads, \n",
    "                               embedding_dim, \n",
    "                               n_layers_decoder, \n",
    "                               dropout_p_decoder, \n",
    "                               ff_hidden_dim_decoder)\n",
    "        self.device = device\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        mask = torch.tril(torch.ones((trg.shape[1], trg.shape[1]))).to(device)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        z = self.encoder(src)\n",
    "        y_pred = self.decoder(z, trg, mask)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_src_len = 300\n",
    "vocab_trg_len = 400\n",
    "max_len = 60\n",
    "batch_size = 300\n",
    "ff_hidden_dim = 1024\n",
    "n_layers_encoder = 3\n",
    "n_layers_decoder = 3\n",
    "\n",
    "transformer = Transformer(\n",
    "    vocab_src_len, \n",
    "    vocab_trg_len, \n",
    "    max_len, \n",
    "    device,\n",
    "    n_layers_encoder=n_layers_encoder,\n",
    "    n_layers_decoder=n_layers_decoder,\n",
    "    ff_hidden_dim_encoder=ff_hidden_dim,\n",
    "    ff_hidden_dim_decoder=ff_hidden_dim,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_test = np.random.randint(0, vocab_src_len, size=(batch_size, max_len))\n",
    "trg_test = np.random.randint(0, vocab_trg_len, size=(batch_size, max_len))\n",
    "\n",
    "src_test = torch.Tensor(src_test).long().to(device)\n",
    "trg_test = torch.Tensor(trg_test).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 60, 400])\n"
     ]
    }
   ],
   "source": [
    "print(transformer(src_test, trg_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
