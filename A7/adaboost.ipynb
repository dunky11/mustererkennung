{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo9GWxKgTN1h"
   },
   "source": [
    "# Mustererkennung/Machine Learning - Assignment 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax8ea49_bkdb"
   },
   "source": [
    "### Load the spam dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T11:28:48.347720Z",
     "start_time": "2018-11-29T11:28:47.572823Z"
    },
    "id": "V7XaSv5wTN1i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T11:28:48.406520Z",
     "start_time": "2018-11-29T11:28:48.349530Z"
    },
    "id": "sT2Hk2k-TN1i"
   },
   "outputs": [],
   "source": [
    "data = np.array(pd.read_csv('spambase.data', header=None))\n",
    "\n",
    "X = data[:,:-1] # features\n",
    "y = data[:,-1] # Last column is label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decision Tree implementation, which will be used in AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def gini(y_true, c):\n",
    "    \"\"\"\n",
    "    For simplicity reasons this assumes that there are only 2 classes\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    p_mk = np.mean(y_true == c)\n",
    "\n",
    "    return 2 * p_mk * (1 - p_mk)\n",
    "\n",
    "class LeafNode():\n",
    "    def fit(self, c):\n",
    "        self.c = c\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.c\n",
    "    \n",
    "class InternalNode():\n",
    "    \"\"\"\n",
    "    Node of decision tree, which accepts tabular data\n",
    "    \"\"\"\n",
    "    def fit(self, x, y, depth, max_depth):\n",
    "        m, n = x.shape\n",
    "        # columns are j, split_index, loss_total\n",
    "        split_infos = []\n",
    "            \n",
    "        for j in range(n):\n",
    "            # sort rows by feature j in ascending order\n",
    "            sorted_indices = x[:,j].argsort()\n",
    "            x, y = x[sorted_indices], y[sorted_indices]\n",
    "            for split_index in self.get_unique_indices(x[:, j])[:-1]:\n",
    "                y_top_split = y[:split_index + 1]\n",
    "                y_bottom_split = y[split_index + 1:]\n",
    "                \n",
    "                if y_top_split.shape[0] == 0:\n",
    "                    raise Exception(\"Error 1\")\n",
    "                    \n",
    "                if y_bottom_split.shape[0] == 0:\n",
    "                    raise Exception(\"Error 2\")\n",
    "                \n",
    "                c1 = self.find_c(y_top_split)\n",
    "                c2 = self.find_c(y_bottom_split)\n",
    "                \n",
    "                loss_1 = gini(y_top_split, c1)\n",
    "                loss_2 = gini(y_bottom_split, c2)\n",
    "                \n",
    "                # use weighted average which had better results\n",
    "                loss_total = (y_top_split.shape[0] / m) * loss_1  + (y_bottom_split.shape[0] / m) * loss_2\n",
    "                \n",
    "                row = np.array([j, split_index, loss_total])\n",
    "                split_infos.append(row)\n",
    "                \n",
    "        split_infos = np.array(split_infos)\n",
    "        best_split_idx = np.argmin(split_infos[:,-1], axis=0)\n",
    "        best_split = split_infos[best_split_idx]\n",
    "        self.j = int(best_split[0])\n",
    "        split_index = int(best_split[1])\n",
    "        \n",
    "        sorted_indices = x[:,j].argsort()\n",
    "        x, y = x[sorted_indices], y[sorted_indices]\n",
    "        \n",
    "        x_top_split, y_top_split = x[:split_index + 1], y[:split_index + 1]\n",
    "        x_bottom_split, y_bottom_split = x[split_index + 1:], y[split_index + 1:]\n",
    "        \n",
    "        self.z = (x_top_split[-1, self.j] + x_bottom_split[0, self.j]) / 2\n",
    "        \n",
    "        if self.is_pure(y_top_split) or x_top_split.shape[0] <= 2 or depth >= max_depth:\n",
    "            self.left_child = LeafNode()\n",
    "            c = self.find_c(y_top_split)\n",
    "            self.left_child.fit(c)\n",
    "        else:\n",
    "            self.left_child = InternalNode()\n",
    "            self.left_child.fit(x_top_split, y_top_split, depth + 1, max_depth)\n",
    "            \n",
    "        if self.is_pure(y_bottom_split) or x_bottom_split.shape[0] <= 2 or depth >= max_depth:\n",
    "            self.right_child = LeafNode()\n",
    "            c = self.find_c(y_bottom_split)\n",
    "            self.right_child.fit(c)\n",
    "        else:\n",
    "            self.right_child = InternalNode()\n",
    "            self.right_child.fit(x_bottom_split, y_bottom_split, depth + 1, max_depth)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        if x[self.j] <= self.z:\n",
    "            return self.left_child.predict(x)\n",
    "        return self.right_child.predict(x)\n",
    "    \n",
    "    def get_unique_indices(self, arr):\n",
    "        idx = []\n",
    "        arr_len = len(arr)\n",
    "        for i in range(len(arr)):\n",
    "            if i == arr_len - 1:\n",
    "                idx.append(i)\n",
    "            elif arr[i] != arr[i + 1]:\n",
    "                idx.append(i)\n",
    "        return idx\n",
    "    \n",
    "    def find_c(self, y):\n",
    "        \"\"\"\n",
    "        For simplicity reasons this assumes that there are only 2 classes\n",
    "        \"\"\"\n",
    "        y = np.array(y)\n",
    "        zeros = np.sum(y == 0)\n",
    "        ones = np.sum(y == 1)\n",
    "        if ones > zeros:\n",
    "            return 1\n",
    "        return 0\n",
    "        \n",
    "    def is_pure(self, y):\n",
    "        y = np.array(y)\n",
    "        if np.sum(y == 0) == y.shape[0] or np.sum(y == 1) == y.shape[0]:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "class DecisionTreeClassifier():\n",
    "    \"\"\"\n",
    "    Basically just holds the root node of the tree which starts the recursion\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, x, y, features):\n",
    "        x = np.copy(x)\n",
    "        y = np.copy(y)\n",
    "        self.root = InternalNode()\n",
    "        self.root.fit(x, y, 1, self.max_depth)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_preds = []\n",
    "        for sample in x:\n",
    "            y_pred = self.root.predict(sample)\n",
    "            y_preds.append(y_pred)\n",
    "        return np.array(y_preds)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1. AdaBoost\n",
    "\n",
    "Implement AdaBoost using Python (incl. Numpy etc.) and use it on the SPAM-Dataset. The weak classifiers should be decision stumps (i.e. decision trees with one node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, num_trees, max_depth):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "            \n",
    "    def fit(self, x, y, features=None):\n",
    "        x = np.copy(x)\n",
    "        y = np.copy(y)\n",
    "        self.trees = []\n",
    "        self.says = []\n",
    "        for i in tqdm(range(self.num_trees)):\n",
    "            # sample weights will always add up to one\n",
    "            sample_weights = np.ones((y.shape[0], )) / y.shape[0]\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            tree.fit(x, y, features)\n",
    "            y_pred = tree.predict(x)\n",
    "            error = self.calculate_error(y, y_pred, sample_weights)\n",
    "            say = self.error_to_say(error)\n",
    "            sample_weights = self.update_sample_weights(y, y_pred, say, sample_weights)\n",
    "            x, y = self.weighted_dataset(x, y, sample_weights)\n",
    "            self.trees.append(tree)\n",
    "            self.says.append(say)\n",
    "            \n",
    "        self.says = np.array(self.says)\n",
    "            \n",
    "    def weighted_dataset(self, x, y, sample_weights):\n",
    "        x_new, y_new = [], []\n",
    "        sample_weights_cum = np.cumsum(sample_weights)\n",
    "        rand = np.random.uniform(low=0.0, high=1.0, size=(x.shape[0], ))\n",
    "        for rand_el in rand:\n",
    "            for i, cum_weight in enumerate(sample_weights_cum):\n",
    "                if cum_weight >= rand_el:\n",
    "                    x_new.append(x[i])\n",
    "                    y_new.append(y[i])\n",
    "                    break\n",
    "        return np.array(x_new), np.array(y_new)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def calculate_error(self, y_true, y_pred, sample_weights):\n",
    "        \"\"\"\n",
    "        How much say a stump has is calculated by it's error, which is just the\n",
    "        sum of the sample_weights for the missclassified samples.\n",
    "        The error is always between 0 and 1 because the sample weights add up to one.\n",
    "        0 is the lowest possible error and 1 is the highest.\n",
    "        \"\"\"\n",
    "        error_idx = y_true != y_pred\n",
    "        return np.sum(sample_weights[error_idx])\n",
    "    \n",
    "    def error_to_say(self, error):\n",
    "        \"\"\"\n",
    "        Transforms the error a stump has into it's say which will be used\n",
    "        to weight the importance of one stumps prediction in the final prediction.\n",
    "        The say is ~ between 3.5 and -3.5 which means a stumps prediction can actually\n",
    "        be weighted negaively in the final prediction if it error is high.\n",
    "        If error is 0 we will have division by 0, if error is 1, we will have log(0)\n",
    "        which is also not possible. So a small eps is added / subtracted from the \n",
    "        error to keep calculations stable.\n",
    "        \"\"\"\n",
    "        eps = 10 ** -10\n",
    "        if error == 0:\n",
    "            error = error + eps\n",
    "        elif error == 1:\n",
    "            error = error - eps\n",
    "        return 0.5 * np.log((1 - error) / error)\n",
    "    \n",
    "    def update_sample_weights(self, y_true, y_pred, say, sample_weights):\n",
    "        \"\"\"\n",
    "        Updates the sample weights by scaling them based on the amount of say the stump\n",
    "        has and wether it properly classified the sample.\n",
    "        If say is high and the sample was missclassified, the sample weight will go up.\n",
    "        If say is high and the sample was propely classified, the sample weight will go down.\n",
    "        If say is low and the sample was missclassified, the sample weight will go down.\n",
    "        If say is low and the sample was properly classified, the sample weight will go up.\n",
    "        After updating the sample weights will still sum up to one.\n",
    "        \"\"\"\n",
    "        sample_weights = np.where(y_true == y_pred, \n",
    "                                  sample_weights * np.exp(-say), \n",
    "                                  sample_weights * np.exp(say))\n",
    "        # normalization so sample_weights add up to 1 again\n",
    "        sample_weights = sample_weights / np.sum(sample_weights)\n",
    "        return sample_weights\n",
    "        \n",
    "    def predict(self, x, use_cascade=False):\n",
    "        y_preds = []\n",
    "        if use_cascade:\n",
    "            for sample in x:\n",
    "                votes = np.array([])\n",
    "                made_prediction = False\n",
    "                for tree in self.trees:\n",
    "                    # decision tree expects matrix as input\n",
    "                    sample = sample.reshape((1, -1))\n",
    "                    prediction = tree.predict(sample)\n",
    "                    votes = np.concatenate((votes, prediction))\n",
    "                    yes_say = np.sum(self.says[:len(votes)][votes == 1])\n",
    "                    no_say = np.sum(self.says[:len(votes)][votes == 0])\n",
    "                    if no_say >= yes_say:\n",
    "                        y_preds.append(0)\n",
    "                        made_prediction = True\n",
    "                        break\n",
    "                if not made_prediction:\n",
    "                    y_preds.append(1)\n",
    "        else:\n",
    "            for sample in x:\n",
    "                votes = np.array([])\n",
    "                for tree in self.trees:\n",
    "                    # decision tree expects matrix as input\n",
    "                    sample = sample.reshape((1, -1))\n",
    "                    prediction = tree.predict(sample)\n",
    "                    votes = np.concatenate((votes, prediction))\n",
    "                yes_say = np.sum(self.says[votes == 1])\n",
    "                no_say = np.sum(self.says[votes == 0])\n",
    "                if yes_say > no_say:\n",
    "                    y_preds.append(1)\n",
    "                else:\n",
    "                    y_preds.append(0)\n",
    "                        \n",
    "        return np.array(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:24<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.1 s, sys: 8.4 ms, total: 24.1 s\n",
      "Wall time: 24.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = AdaBoost(num_trees=20, max_depth=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using AdaBoost to predict on the SPAM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 91.4857%\n",
      "CPU times: user 139 ms, sys: 8.05 ms, total: 147 ms\n",
      "Wall time: 138 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(f\"Accuracy of {round(100 * acc, 4)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a) Print a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[656  41]\n",
      " [ 57 397]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b) Is AdaBoost better when using stronger weak learners? Why or why not? Compare your results to using depth-2 decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:34<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.6 s, sys: 132 ms, total: 34.7 s\n",
      "Wall time: 34.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = AdaBoost(num_trees=20, max_depth=2) \n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 90.3562%\n",
      "CPU times: user 156 ms, sys: 1e+03 ns, total: 156 ms\n",
      "Wall time: 155 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(f\"Accuracy of {round(100 * acc, 4)}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ensembles.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
