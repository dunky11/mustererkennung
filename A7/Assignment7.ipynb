{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo9GWxKgTN1h"
   },
   "source": [
    "# Mustererkennung/Machine Learning - Assignment 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax8ea49_bkdb"
   },
   "source": [
    "### Load the spam dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T11:28:48.347720Z",
     "start_time": "2018-11-29T11:28:47.572823Z"
    },
    "id": "V7XaSv5wTN1i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.data import lfw_subset\n",
    "from sklearn.datasets import fetch_openml\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T11:28:48.406520Z",
     "start_time": "2018-11-29T11:28:48.349530Z"
    },
    "id": "sT2Hk2k-TN1i"
   },
   "outputs": [],
   "source": [
    "data = np.array(pd.read_csv('spambase.data', header=None))\n",
    "\n",
    "X = data[:,:-1] # features\n",
    "y = data[:,-1] # Last column is label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def gini(y_true, c):\n",
    "    \"\"\"\n",
    "    For simplicity reasons this assumes that there are only 2 classes\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    p_mk = np.mean(y_true == c)\n",
    "\n",
    "    return 2 * p_mk * (1 - p_mk)\n",
    "\n",
    "class LeafNode():\n",
    "    def fit(self, c):\n",
    "        self.c = c\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.c\n",
    "    \n",
    "class InternalNode():\n",
    "    \"\"\"\n",
    "    Node of decision tree, which accepts tabular data\n",
    "    \"\"\"\n",
    "    def fit(self, x, y, depth, max_depth):\n",
    "        m, n = x.shape\n",
    "        # columns are j, split_index, loss_total\n",
    "        split_infos = []\n",
    "            \n",
    "        for j in range(n):\n",
    "            # sort rows by feature j in ascending order\n",
    "            sorted_indices = x[:,j].argsort()\n",
    "            x, y = x[sorted_indices], y[sorted_indices]\n",
    "            for split_index in self.get_unique_indices(x[:, j])[:-1]:\n",
    "                y_top_split = y[:split_index + 1]\n",
    "                y_bottom_split = y[split_index + 1:]\n",
    "                \n",
    "                if y_top_split.shape[0] == 0:\n",
    "                    raise Exception(\"Error 1\")\n",
    "                    \n",
    "                if y_bottom_split.shape[0] == 0:\n",
    "                    raise Exception(\"Error 2\")\n",
    "                \n",
    "                c1 = self.find_c(y_top_split)\n",
    "                c2 = self.find_c(y_bottom_split)\n",
    "                \n",
    "                loss_1 = gini(y_top_split, c1)\n",
    "                loss_2 = gini(y_bottom_split, c2)\n",
    "                \n",
    "                # use weighted average which had better results\n",
    "                loss_total = (y_top_split.shape[0] / m) * loss_1  + (y_bottom_split.shape[0] / m) * loss_2\n",
    "                \n",
    "                row = np.array([j, split_index, loss_total])\n",
    "                split_infos.append(row)\n",
    "                \n",
    "        split_infos = np.array(split_infos)\n",
    "        best_split_idx = np.argmin(split_infos[:,-1], axis=0)\n",
    "        best_split = split_infos[best_split_idx]\n",
    "        self.j = int(best_split[0])\n",
    "        split_index = int(best_split[1])\n",
    "        \n",
    "        sorted_indices = x[:,j].argsort()\n",
    "        x, y = x[sorted_indices], y[sorted_indices]\n",
    "        \n",
    "        x_top_split, y_top_split = x[:split_index + 1], y[:split_index + 1]\n",
    "        x_bottom_split, y_bottom_split = x[split_index + 1:], y[split_index + 1:]\n",
    "        \n",
    "        self.z = (x_top_split[-1, self.j] + x_bottom_split[0, self.j]) / 2\n",
    "        \n",
    "        if self.is_pure(y_top_split) or x_top_split.shape[0] <= 2 or depth >= max_depth:\n",
    "            self.left_child = LeafNode()\n",
    "            c = self.find_c(y_top_split)\n",
    "            self.left_child.fit(c)\n",
    "        else:\n",
    "            self.left_child = InternalNode()\n",
    "            self.left_child.fit(x_top_split, y_top_split, depth + 1, max_depth)\n",
    "            \n",
    "        if self.is_pure(y_bottom_split) or x_bottom_split.shape[0] <= 2 or depth >= max_depth:\n",
    "            self.right_child = LeafNode()\n",
    "            c = self.find_c(y_bottom_split)\n",
    "            self.right_child.fit(c)\n",
    "        else:\n",
    "            self.right_child = InternalNode()\n",
    "            self.right_child.fit(x_bottom_split, y_bottom_split, depth + 1, max_depth)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        if x[self.j] <= self.z:\n",
    "            return self.left_child.predict(x)\n",
    "        return self.right_child.predict(x)\n",
    "    \n",
    "    def get_unique_indices(self, arr):\n",
    "        idx = []\n",
    "        arr_len = len(arr)\n",
    "        for i in range(len(arr)):\n",
    "            if i == arr_len - 1:\n",
    "                idx.append(i)\n",
    "            elif arr[i] != arr[i + 1]:\n",
    "                idx.append(i)\n",
    "        return idx\n",
    "    \n",
    "    def find_c(self, y):\n",
    "        \"\"\"\n",
    "        For simplicity reasons this assumes that there are only 2 classes\n",
    "        \"\"\"\n",
    "        y = np.array(y)\n",
    "        zeros = np.sum(y == 0)\n",
    "        ones = np.sum(y == 1)\n",
    "        if ones > zeros:\n",
    "            return 1\n",
    "        return 0\n",
    "        \n",
    "    def is_pure(self, y):\n",
    "        y = np.array(y)\n",
    "        if np.sum(y == 0) == y.shape[0] or np.sum(y == 1) == y.shape[0]:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "class DecisionTreeClassifier():\n",
    "    \"\"\"\n",
    "    Basically just holds the root node of the tree which starts the recursion\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, x, y, features):\n",
    "        x = np.copy(x)\n",
    "        y = np.copy(y)\n",
    "        self.root = InternalNode()\n",
    "        self.root.fit(x, y, 1, self.max_depth)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_preds = []\n",
    "        for sample in x:\n",
    "            y_pred = self.root.predict(sample)\n",
    "            y_preds.append(y_pred)\n",
    "        return np.array(y_preds)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Delete later\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class AdaBoost():\n",
    "    def __init__(self, num_trees, max_depth):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "            \n",
    "    def fit(self, x, y, features=None):\n",
    "        x = np.copy(x)\n",
    "        y = np.copy(y)\n",
    "        self.trees = []\n",
    "        self.says = []\n",
    "        for i in tqdm(range(self.num_trees)):\n",
    "            # sample weights will always add up to one\n",
    "            sample_weights = np.ones((y.shape[0], )) / y.shape[0]\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            tree.fit(x, y, features)\n",
    "            y_pred = tree.predict(x)\n",
    "            error = self.calculate_error(y, y_pred, sample_weights)\n",
    "            say = self.error_to_say(error)\n",
    "            sample_weights = self.update_sample_weights(y, y_pred, say, sample_weights)\n",
    "            x, y = self.weighted_dataset(x, y, sample_weights)\n",
    "            self.trees.append(tree)\n",
    "            self.says.append(say)\n",
    "            \n",
    "        self.says = np.array(self.says)\n",
    "            \n",
    "    def weighted_dataset(self, x, y, sample_weights):\n",
    "        x_new, y_new = [], []\n",
    "        sample_weights_cum = np.cumsum(sample_weights)\n",
    "        rand = np.random.uniform(low=0.0, high=1.0, size=(x.shape[0], ))\n",
    "        for rand_el in rand:\n",
    "            for i, cum_weight in enumerate(sample_weights_cum):\n",
    "                if cum_weight >= rand_el:\n",
    "                    x_new.append(x[i])\n",
    "                    y_new.append(y[i])\n",
    "                    break\n",
    "        return np.array(x_new), np.array(y_new)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def calculate_error(self, y_true, y_pred, sample_weights):\n",
    "        \"\"\"\n",
    "        How much say a stump has is calculated by it's error, which is just the\n",
    "        sum of the sample_weights for the missclassified samples.\n",
    "        The error is always between 0 and 1 because the sample weights add up to one.\n",
    "        0 is the lowest possible error and 1 is the highest.\n",
    "        \"\"\"\n",
    "        error_idx = y_true != y_pred\n",
    "        return np.sum(sample_weights[error_idx])\n",
    "    \n",
    "    def error_to_say(self, error):\n",
    "        \"\"\"\n",
    "        Transforms the error a stump has into it's say which will be used\n",
    "        to weight the importance of one stumps prediction in the final prediction.\n",
    "        The say is ~ between 3.5 and -3.5 which means a stumps prediction can actually\n",
    "        be weighted negaively in the final prediction if it error is high.\n",
    "        If error is 0 we will have division by 0, if error is 1, we will have log(0)\n",
    "        which is also not possible. So a small eps is added / subtracted from the \n",
    "        error to keep calculations stable.\n",
    "        \"\"\"\n",
    "        eps = 10 ** -10\n",
    "        if error == 0:\n",
    "            error = error + eps\n",
    "        elif error == 1:\n",
    "            error = error - eps\n",
    "        return 0.5 * np.log((1 - error) / error)\n",
    "    \n",
    "    def update_sample_weights(self, y_true, y_pred, say, sample_weights):\n",
    "        \"\"\"\n",
    "        Updates the sample weights by scaling them based on the amount of say the stump\n",
    "        has and wether it properly classified the sample.\n",
    "        If say is high and the sample was missclassified, the sample weight will go up.\n",
    "        If say is high and the sample was propely classified, the sample weight will go down.\n",
    "        If say is low and the sample was missclassified, the sample weight will go down.\n",
    "        If say is low and the sample was properly classified, the sample weight will go up.\n",
    "        After updating the sample weights will still sum up to one.\n",
    "        \"\"\"\n",
    "        sample_weights = np.where(y_true == y_pred, \n",
    "                                  sample_weights * np.exp(-say), \n",
    "                                  sample_weights * np.exp(say))\n",
    "        # normalization so sample_weights add up to 1 again\n",
    "        sample_weights = sample_weights / np.sum(sample_weights)\n",
    "        return sample_weights\n",
    "        \n",
    "    def predict(self, x):\n",
    "        y_preds = []\n",
    "        for sample in x:\n",
    "            votes = []\n",
    "            for tree in self.trees:\n",
    "                # decision tree expects matrix as input\n",
    "                sample = sample.reshape((1, -1))\n",
    "                votes.append(tree.predict(sample))\n",
    "            votes = np.array(votes).reshape((-1, ))\n",
    "            yes_say = np.sum(self.says[votes == 1])\n",
    "            no_say = np.sum(self.says[votes == 0])\n",
    "            if yes_say > no_say:\n",
    "                y_preds.append(1)\n",
    "            else:\n",
    "                y_preds.append(0)\n",
    "        return np.array(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b30c35640a8472083ae8da96290a7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 28.2 s, sys: 7.74 ms, total: 28.2 s\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = AdaBoost(num_trees=20, max_depth=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9035621198957429\n",
      "CPU times: user 101 ms, sys: 7 µs, total: 101 ms\n",
      "Wall time: 100 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to AdaBoost with depth of 2 instead of 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4436eff50bb04438b0d2ff7f0c09df4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 38.4 s, sys: 19.8 ms, total: 38.4 s\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = AdaBoost(num_trees=20, max_depth=2)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8696785403996524\n",
      "CPU times: user 117 ms, sys: 5 µs, total: 117 ms\n",
      "Wall time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Viola-Jones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    \"\"\"\n",
    "    Generates dataset of 250 images of faces with labels 1 and 250 images of digits with label 0, \n",
    "    each are 25x25 pixels.\n",
    "    \"\"\"\n",
    "    face_data = lfw_subset()\n",
    "    mnist = fetch_openml('mnist_784').data[:face_data.shape[0]]\n",
    "    mnist = mnist.reshape((-1, 28, 28))\n",
    "    mnist = mnist[:,1:26, 1:26] / 255.0\n",
    "    face_labels = np.ones((face_data.shape[0], ))\n",
    "    mnist_labels = np.zeros((mnist.shape[0], ))\n",
    "    x = np.vstack((face_data, mnist))\n",
    "    y = np.hstack((face_labels, mnist_labels))\n",
    "    assert(x.shape[0] == y.shape[0])\n",
    "    return x, y\n",
    "\n",
    "x, y = generate_dataset()\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RectBase():\n",
    "    def calculate_area(self, integral_img,  ul_pixel_cord, lr_pixel_cord):\n",
    "        \"\"\" Calculates the of the integral_img given by two pixel coordinates.\n",
    "        ul_pixel_cord should be outside of the calculated area.\n",
    "        lr_pixel_cord is inside of the calculated area.\n",
    "        \"\"\"\n",
    "        if ul_pixel_cord[0] < 0 or ul_pixel_cord[1] < 0:\n",
    "            A = 0\n",
    "        else:\n",
    "            A = integral_img[ul_pixel_cord[0], ul_pixel_cord[1]]\n",
    "            \n",
    "        if ul_pixel_cord[0] < 0:\n",
    "            B = 0\n",
    "        else:\n",
    "            B = integral_img[ul_pixel_cord[0], lr_pixel_cord[1]]\n",
    "            \n",
    "        if ul_pixel_cord[1] < 0:\n",
    "            C = 0\n",
    "        else:\n",
    "            C = integral_img[ul_pixel_cord[1], lr_pixel_cord[0]]\n",
    "            \n",
    "        D = integral_img[lr_pixel_cord[0], lr_pixel_cord[1]]\n",
    "        return A - B - C + D\n",
    "\n",
    "\n",
    "class TwoRect(RectBase):\n",
    "    \"\"\" Feature for Viola-Jones, which looks like:\n",
    "    -----\n",
    "    --B--\n",
    "    --W--\n",
    "    -----\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, scale_x, scale_y, img_shape):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.scale_x = scale_x\n",
    "        self.scale_y = scale_y\n",
    "        self.img_shape = img_shape\n",
    "    \n",
    "    def is_valid(self):\n",
    "        \"\"\"\n",
    "        Returns wether the feature fits on the image\n",
    "        \"\"\"\n",
    "        lowest_pixel = self.x + (self.scale_x * 2) - 1\n",
    "        if lowest_pixel > self.img_shape[0] - 1:\n",
    "            return False\n",
    "        most_right_pixel = self.y + self.scale_y - 1\n",
    "        if most_right_pixel > self.img_shape[1] - 1:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def get_pixel_cords(self):\n",
    "        black_area_ul = [self.x - 1, self.y - 1]\n",
    "        black_area_lr = [self.x + self.scale_x - 1, self.y + self.scale_y - 1]\n",
    "        white_area_ul = [self.x + self.scale_x - 1, self.y - 1]\n",
    "        white_area_lr = [self.x + (self.scale_x * 2) - 1, self.y + self.scale_y - 1]\n",
    "        return {\n",
    "            \"black_area_ul\": black_area_ul,\n",
    "            \"black_area_lr\": black_area_lr,\n",
    "            \"white_area_ul\": white_area_ul,\n",
    "            \"white_area_lr\": white_area_lr\n",
    "        }\n",
    "    \n",
    "    def apply_feature(self, integral_img):\n",
    "        pixel_cords = self.get_pixel_cords()\n",
    "        black_area_sum = self.calculate_area(integral_img, pixel_cords[\"black_area_ul\"], \n",
    "                                             pixel_cords[\"black_area_lr\"])\n",
    "        white_area_sum = self.calculate_area(integral_img, pixel_cords[\"white_area_ul\"], \n",
    "                                             pixel_cords[\"white_area_lr\"])\n",
    "        return white_area_sum - black_area_sum\n",
    "    \n",
    "    def visualise_feature(self):\n",
    "        img = np.ones((self.img_shape[0], self.img_shape[1])) * 0.5\n",
    "        pixel_cords = self.get_pixel_cords()\n",
    "        print(pixel_cords) \n",
    "        for x in range(pixel_cords[\"black_area_ul\"][0] + 1, pixel_cords[\"black_area_lr\"][0] + 1):\n",
    "            for y  in range(pixel_cords[\"black_area_ul\"][1] + 1, pixel_cords[\"black_area_lr\"][1] + 1):\n",
    "                img[x, y] = 0\n",
    "        for x in range(pixel_cords[\"white_area_ul\"][0] + 1, pixel_cords[\"white_area_lr\"][0] + 1):\n",
    "            for y  in range(pixel_cords[\"white_area_ul\"][1] + 1, pixel_cords[\"white_area_lr\"][1] + 1):\n",
    "                img[x, y] = 1\n",
    "        return img\n",
    "\n",
    "class ThreeRect(RectBase):\n",
    "    \"\"\" Feature for Viola-Jones, which looks like:\n",
    "    -----\n",
    "    -BWB-\n",
    "    -----\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, scale_x, scale_y, img_shape):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.scale_x = scale_x\n",
    "        self.scale_y = scale_y\n",
    "        self.img_shape = img_shape\n",
    "    \n",
    "    def is_valid(self):\n",
    "        \"\"\"\n",
    "        Returns wether the feature fits on the image\n",
    "        \"\"\"\n",
    "        lowest_pixel = self.x + self.scale_x - 1\n",
    "        if lowest_pixel > self.img_shape[0] - 1:\n",
    "            return False\n",
    "        most_right_pixel = self.y + (self.scale_y * 3) - 1\n",
    "        if most_right_pixel > self.img_shape[1] - 1:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def get_pixel_cords(self):\n",
    "        left_black_area_ul = [self.x - 1, self.y - 1]\n",
    "        left_black_area_lr = [self.x + self.scale_x - 1, self.y + self.scale_y - 1]\n",
    "        white_area_ul = [self.x - 1, self.y + self.scale_y - 1]\n",
    "        white_area_lr = [self.x + self.scale_x - 1, self.y + self.scale_y * 2 - 1]\n",
    "        right_black_area_ul = [self.x - 1, self.y + self.scale_y * 2 - 1]\n",
    "        right_black_area_lr = [self.x + self.scale_x - 1, self.y + (self.scale_y * 3) - 1]\n",
    "        return {\n",
    "            \"left_black_area_ul\": left_black_area_ul,\n",
    "            \"left_black_area_lr\": left_black_area_lr,\n",
    "            \"white_area_ul\": white_area_ul,\n",
    "            \"white_area_lr\": white_area_lr, \n",
    "            \"right_black_area_ul\": right_black_area_ul,\n",
    "            \"right_black_area_lr\": right_black_area_lr\n",
    "        }\n",
    "    \n",
    "    def apply_feature(self, integral_img):\n",
    "        pixel_cords = self.get_pixel_cords()\n",
    "        left_black_area_sum = self.calculate_area(integral_img, pixel_cords[\"left_black_area_ul\"], \n",
    "                                             pixel_cords[\"left_black_area_lr\"])\n",
    "        white_area_sum = self.calculate_area(integral_img, pixel_cords[\"white_area_ul\"], \n",
    "                                             pixel_cords[\"white_area_lr\"])\n",
    "        right_black_area_sum = self.calculate_area(integral_img, pixel_cords[\"right_black_area_ul\"], \n",
    "                                             pixel_cords[\"right_black_area_lr\"])\n",
    "        return white_area_sum - (left_black_area_sum + right_black_area_sum)\n",
    "    \n",
    "    def visualise_feature(self):\n",
    "        img = np.ones((self.img_shape[0], self.img_shape[1])) * 0.5\n",
    "        pixel_cords = self.get_pixel_cords()\n",
    "        print(pixel_cords)\n",
    "        for x in range(pixel_cords[\"left_black_area_ul\"][0] + 1, pixel_cords[\"left_black_area_lr\"][0] + 1):\n",
    "            for y  in range(pixel_cords[\"left_black_area_ul\"][1] + 1, pixel_cords[\"left_black_area_lr\"][1] + 1):\n",
    "                img[x, y] = 0\n",
    "        for x in range(pixel_cords[\"white_area_ul\"][0] + 1, pixel_cords[\"white_area_lr\"][0] + 1):\n",
    "            for y  in range(pixel_cords[\"white_area_ul\"][1] + 1, pixel_cords[\"white_area_lr\"][1] + 1):\n",
    "                img[x, y] = 1\n",
    "        for x in range(pixel_cords[\"right_black_area_ul\"][0] + 1, pixel_cords[\"right_black_area_lr\"][0] + 1):\n",
    "            for y  in range(pixel_cords[\"right_black_area_ul\"][1] + 1, pixel_cords[\"right_black_area_lr\"][1] + 1):\n",
    "                img[x, y] = 0\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViolaJonesClassifier():\n",
    "    def __init__(self, num_trees, max_depth, max_features=None):\n",
    "        self.adaboost = AdaBoost(num_trees, max_depth)\n",
    "        self.max_features = max_features\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        x = np.copy(x)\n",
    "        y = np.copy(y)\n",
    "        features = self.generate_features(x[0].shape)\n",
    "        print(len(features))\n",
    "        if self.max_features != None:\n",
    "            random.shuffle(features)\n",
    "            features = features[:self.max_features]\n",
    "        x = self.transform_x(x, features)\n",
    "        self.adaboost.fit(x, y)\n",
    "        self.features = features\n",
    "        \n",
    "    def predict(self, x):\n",
    "        x = np.copy(x)\n",
    "        x = self.transform_x(x, self.features)\n",
    "        return self.adaboost.predict(x)\n",
    "    \n",
    "    def transform_x(self, x, features):\n",
    "        imgs = []\n",
    "        for sample in x:\n",
    "            imgs.append(self.to_integral_img(sample))\n",
    "        x = np.array(imgs)\n",
    "        x = self.apply_features(x, features)\n",
    "        return x\n",
    "    \n",
    "    def apply_features(self, integral_imgs, features):\n",
    "        \"\"\"\n",
    "        Transforms the integral images into a tabular dataset of shape (num_imgs, num_features),\n",
    "        where [i, j] corresponds to the j-th feature applied to the i-th img\n",
    "        \"\"\"\n",
    "        x_new = np.zeros((integral_imgs.shape[0], len(features)))\n",
    "        for row in range(x_new.shape[0]):\n",
    "            for feature in range(len(features)):\n",
    "                x_new[row, feature] = features[feature].apply_feature(integral_imgs[row])\n",
    "        return x_new\n",
    "    \n",
    "    def to_integral_img(self, img):\n",
    "        img = img.copy()\n",
    "        for row in range(img.shape[0]):\n",
    "            for col in range(img.shape[1]):\n",
    "                new_val = img[row, col]\n",
    "                if row != 0:\n",
    "                    new_val += img[row - 1, col]\n",
    "                if col != 0:\n",
    "                    new_val += img[row, col - 1]\n",
    "                if row != 0 and col != 0:\n",
    "                    new_val -= img[row - 1, col - 1]\n",
    "                img[row, col] = new_val\n",
    "        return img\n",
    "    \n",
    "    def generate_features(self, img_shape):\n",
    "        features = []\n",
    "        for x in range(img_shape[0]):\n",
    "            for y in range(img_shape[1]):\n",
    "                for scale_x in range(1, img_shape[0]):\n",
    "                    for scale_y in range(1, img_shape[1]):\n",
    "                        feat = TwoRect(x, y, scale_x, scale_y, img_shape)\n",
    "                        if feat.is_valid():\n",
    "                            features.append(feat)\n",
    "                        feat = ThreeRect(x, y, scale_x, scale_y, img_shape)\n",
    "                        if feat.is_valid():\n",
    "                            features.append(feat)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'black_area_ul': [-1, 0], 'black_area_lr': [1, 3], 'white_area_ul': [1, 0], 'white_area_lr': [3, 3]}\n",
      "[[ 1.  2.  3.  4.  5.  6.]\n",
      " [ 2.  4.  6.  8. 10. 12.]\n",
      " [ 3.  6.  9. 12. 15. 18.]\n",
      " [ 4.  8. 12. 16. 20. 24.]\n",
      " [ 5. 10. 15. 20. 25. 30.]\n",
      " [ 6. 12. 18. 24. 30. 36.]]\n",
      "0.0\n",
      "Its a valid feature\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAALAklEQVR4nO3db4hldR3H8c/H0UjN8EEh4gqrEBEIqSxCLEiZylpiPeiBgj6IYCUqjIKonhzPg56KQRBd1DIyRdSlELOEEhPKnPVPua6FmOBgsYmJ2YNE/fRg7qVp3d1z7sy5c+5+eb/gsnNnf3Pmq8x7zj3nnuU4iQDUdMLYAwBYHAIHCiNwoDACBwojcKAwAgcK6xW47T22/2z7edvfXPRQAIbhrvfBba9I+oukyyStSXpc0jVJnl38eAC2os8e/CJJzyd5Icmbku6S9JnFjgVgCH0CP0vSSxuer00/939s77W9ant1MplEEg8ePBb0aNs26uHEHmt8hM+9a+NJJpIms6dt2/b5/gvXNI0kyT7Sf8b2mx0SMc/RzWZatp+hZZunjz578DVJZ294vkPSy3POBGAEfQJ/XNKHbJ9j+z2Srpb088WOBWAInS/Rk7xl+8uSfilpRdJtSQ4sfDIAW9bnGFxJHpD0wIJnATAwrmQDCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwrrDNz2bbYP2X5mOwYCMJw+e/AfSdqz4DkALEBn4EkekfTqNswCYGBO0r3I3inp/iTn9dxu90YBbFrbtmqaxl3rBjvJZnuv7VXbq5PJZKjNAtiCE4faUJKJpFnZadt2qE1vSdM0kiS785fdtpi9YmKeo5vNtGw/Q8s2Tx+8TQYU1udtsjsl/U7Sh22v2f7C4scCMITOl+hJrtmOQQAMj5foQGEEDhRG4EBhBA4URuBAYQQOFEbgQGEEDhRG4EBhBA4URuBAYQQOFEbgQGEEDhRG4EBhBA4URuBAYQQOFEbgQGEEDhRG4EBhBA4URuBAYQQOFEbgQGEEDhRG4EBhBA4U1ufuomfb/o3tg7YP2L5hOwYDsHWddxeV9Jakryd5wvZpkvbbfijJswueDcAWOcl8X2D/TNL3kjx0jGXzbRTAXNq2VdM07lo31zG47Z2SLpD02BH+bq/tVdurk8lkns0CWJA+L9ElSbbfJ+leSV9N8vrhf59kImlWdtq2HWbCLWqaZuwRsEnL9jO0bPP00WsPbvskrcd9R5L7NjkXgG3W5yy6Jd0q6WCSmxY/EoCh9NmD75Z0naRLbD81fXxqwXMBGEDnMXiSRyV1nq0DsHy4kg0ojMCBwggcKIzAgcIIHCiMwIHCCBwojMCBwggcKIzAgcIIHCiMwIHCCBwojMCBwggcKIzAgcIIHCiMwIHCCBwojMCBwggcKIzAgcIIHCiMwIHCCBwojMCBwggcKKzP3UXfa/sPtp+2fcD2ctwkGUCnzpsPSvqPpEuSvDG9T/ijtn+R5PcLng3AFjlJ/8X2KZIelfTFJI8dY2n/jQKYW9u2apqm866/vY7Bba/YfkrSIUkPHSlu23ttr9penUwm808MYHB9XqIryduSzrd9uqR9ts9L8sxhayaSZmWnbZfjUL1pmrFHwCYt28/Qss3Tx1xn0ZO8JulhSXvmGwnAGPqcRf/gdM8t2ydLulTSc4seDMDW9XmJfqak222vaP0Xwt1J7l/sWACG0Bl4kj9KumAbZgEwMK5kAwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcK6x247RXbT9rmxoPAcWKePfgNkg4uahAAw3OS7kX2Dkm3S/qOpK8lubLjS7o3CmDT2rZV0zTuWtd3D36zpG9IeudoC2zvtb1qe3UymfTcLIBF6rw/uO0rJR1Kst/2x4+2LslE0qzstG07zIRb1DSNpPXfeMuAebot20zLOk8fffbguyVdZftFSXdJusT2TzY3GoDt1Bl4km8l2ZFkp6SrJf06ybULnwzAlvE+OFBY5zH4RkkelvTwQiYBMDj24EBhBA4URuBAYQQOFEbgQGEEDhRG4EBhBA4URuBAYQQOFEbgQGEEDhRG4EBhBA4URuBAYQQOFEbgQGEEDhRG4EBhBA4URuBAYQQOFEbgQGEEDhRG4EBhBA4URuBAYb3uTTa9dfC/JL0t6a0kuxY5FIBhzHPzwU8keWVhkwAYHC/RgcKcpHuR/VdJ/5QUST9IMun4ku6NAti0tm3VNI271vXdg+9OcqGkKyR9yfbFhy+wvdf2qu3VyaSrfwDbodcxeJKXp38esr1P0kWSHjlszUTSrOy0bTvknJvWNI2k9d94y4B5ui3bTMs6Tx+de3Dbp9o+bfaxpMslPbPp6QBsmz578DMk7bM9W//TJA8udCoAg+gMPMkLkj66DbMAGBhvkwGFEThQGIEDhRE4UBiBA4UROFAYgQOFEThQGIEDhRE4UBiBA4UROFAYgQOFEThQGIEDhRE4UBiBA4UROFAYgQOFEThQGIEDhRE4UBiBA4UROFAYgQOFEThQGIEDhRE4UFivwG2fbvse28/ZPmj7Y4seDMDW9bl9sCR9V9KDST5n+z2STlngTAAG4iTHXmC/X9LTks5N1+L/6bsOwCa0baumady1rs9L9HMl/UPSD20/afsW26cevsj2Xtur08dPJHlZHravH3sG5jm+Z1q2eW688cbr1UOfwE+UdKGk7ye5QNK/JX3z8EVJJkl2Jdkl6SN9vvk22jv2AIdhnm7LNtNxOU+fwNckrSV5bPr8Hq0HD2DJdQae5O+SXrL94emnPinp2YVOBWAQfc+if0XSHdMz6C9I+nzH+smWphoe8xzbss0jLd9Mx+U8nWfRARy/uJINKIzAgcIGDdz2Htt/tv287Xe9lbbdbN9m+5DtZ8aeRZJsn237N9PLfQ/YvmHked5r+w+2n57O0445z4ztlek1F/cvwSwv2v6T7adsr449j6S5Lh0f7Bjc9oqkv0i6TOtvrT0u6Zoko51xt32xpDck/TjJeWPNsWGeMyWdmeQJ26dJ2i/ps2P9P7JtSacmecP2SZIelXRDkt+PMc+Gub4maZek9ye5cuRZXpS0K8krY86xke3bJf02yS2zS8eTvHaktUPuwS+S9HySF5K8KekuSZ8ZcPtzS/KIpFfHnGGjJH9L8sT0439JOijprBHnSZI3pk9Pmj5GPetqe4ekT0u6Zcw5ltX00vGLJd0qSUnePFrc0rCBnyXppQ3P1zTiD++ys71T0gWSHjv2yoXPsWL7KUmHJD204YKmsdws6RuS3hl5jplI+pXt/baX4Wq2XpeOzwwZuI/wOd6DOwLb75N0r6SvJnl9zFmSvJ3kfEk7JF1ke7RDGdtXSjqUZP9YMxzB7iQXSrpC0pemh31j6nXp+MyQga9JOnvD8x2SXh5w+yVMj3XvlXRHkvvGnmdm+jLvYUl7Rhxjt6Srpse9d0m6ZPoPl0aT5OXpn4ck7dP6oeiY5rp0fMjAH5f0IdvnTA/8r5b08wG3f9ybntS6VdLBJDctwTwftH369OOTJV0q6bmx5knyrSQ7kuzU+s/Pr5NcO9Y8tk+dngzV9GXw5ZJGfUdm3kvH+16q2ucbv2X7y5J+KWlF0m1JDgy1/c2wfaekj0v6gO01SU2SW0ccabek6yT9aXrcK0nfTvLASPOcKen26TsgJ0i6O8nob00tkTMk7Vv/vawTJf00yYPjjiRpjkvHuVQVKIwr2YDCCBwojMCBwggcKIzAgcIIHCiMwIHC/guRlcNyE2oE0QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat = TwoRect(0, 1, 2, 3, (6, 6))\n",
    "visualization = feat.visualise_feature()\n",
    "extent = (0, visualization.shape[1], visualization.shape[0], 0)\n",
    "_, ax = plt.subplots()\n",
    "ax.imshow(visualization, extent=extent, cmap='gray')\n",
    "ax.grid(color='w', linewidth=2)\n",
    "ax.set_frame_on(False)\n",
    "img = np.ones((6, 6))\n",
    "clf = ViolaJonesClassifier(10, 5)\n",
    "integral_img = clf.to_integral_img(img)\n",
    "print(integral_img)\n",
    "print(feat.apply_feature(integral_img))\n",
    "if feat.is_valid():\n",
    "    print(\"Its a valid feature\")\n",
    "else:\n",
    "    print(\"Invalid feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'left_black_area_ul': [-1, -1], 'left_black_area_lr': [0, 0], 'white_area_ul': [-1, 0], 'white_area_lr': [0, 1], 'right_black_area_ul': [-1, 1], 'right_black_area_lr': [0, 2]}\n",
      "[[ 1.  2.  3.  4.  5.  6.]\n",
      " [ 2.  4.  6.  8. 10. 12.]\n",
      " [ 3.  6.  9. 12. 15. 18.]\n",
      " [ 4.  8. 12. 16. 20. 24.]\n",
      " [ 5. 10. 15. 20. 25. 30.]\n",
      " [ 6. 12. 18. 24. 30. 36.]]\n",
      "-1.0\n",
      "Its a valid feature\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat = ThreeRect(0, 0, 1, 1, (6, 6))\n",
    "visualization = feat.visualise_feature()\n",
    "extent = (0, visualization.shape[1], visualization.shape[0], 0)\n",
    "_, ax = plt.subplots()\n",
    "ax.imshow(visualization, extent=extent, cmap='gray')\n",
    "ax.grid(color='w', linewidth=2)\n",
    "ax.set_frame_on(False)\n",
    "img = np.ones((6, 6))\n",
    "clf = ViolaJonesClassifier(10, 5)\n",
    "integral_img = clf.to_integral_img(img)\n",
    "print(integral_img)\n",
    "print(feat.apply_feature(integral_img))\n",
    "if feat.is_valid():\n",
    "    print(\"Its a valid feature\")\n",
    "else:\n",
    "    print(\"Invalid feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1a30d885a6442cb43a2dc314ec1dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 4s, sys: 148 ms, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = ViolaJonesClassifier(num_trees=10, max_depth=1, max_features=1000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n",
      "CPU times: user 565 ms, sys: 4 ms, total: 569 ms\n",
      "Wall time: 566 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ensembles.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
